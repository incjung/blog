---
title: "Spark S3-compatible Options"
date: 2025-09-05T16:07:07+12:00
draft: false
---

* S3 and Spark: Navigating the Configuration Rabbit Hole
Many Spark users find themselves scratching their heads when interacting with AWS S3, especially when deviating from the standard AWS setup. I've certainly been there! While S3 is a powerful and ubiquitous object store, getting Spark to play nicely with it, particularly when using alternatives like MinIO, can be a frustrating experience.

Like many, I've leaned on MinIO for local development and testing. It mimics the S3 API, offering a convenient way to avoid direct AWS interactions. However, transitioning from a standard AWS S3 connection to MinIO within a Spark environment can introduce unexpected challenges.

Recently, I encountered a roadblock while attempting to connect Iceberg to S3 via Spark. Despite seemingly straightforward configuration, S3 connection issues surfaced, highlighting the nuances involved. Even setting environment variables (=AWS_SECRET_ACCESS_KEY=, =AWS_ACCESS_KEY_ID=) as expected didn't guarantee a seamless connection.
#+begin_src bash
   export AWS_SECRET_ACCESS_KEY=m1n1opwd
   export AWS_ACCESS_KEY_ID=minio_root
#+end_src


It felt like falling down a configuration rabbit hole! The subtle differences in connection strings and expected behavior between standard AWS S3 and MinIO tripped me up.

To avoid future headaches, I'm documenting the specific Spark configuration options required for a successful connection, especially when using MinIO or similar S3-compatible storage:

#+begin_src bash
./spark-sql \
    --conf spark.hadoop.fs.s3a.endpoint=http://<minio-address>:9000 \
    --conf spark.hadoop.fs.s3a.access.key=minio_root \
    --conf spark.hadoop.fs.s3a.secret.key=m1n1opwd \
    --conf spark.hadoop.fs.s3a.path.style.access=true \
    ...other Spark options...
#+end_src

** Key configuration options:

- =spark.hadoop.fs.s3a.endpoint=:  This is crucial when not using AWS S3. It explicitly tells Spark the address of your S3-compatible storage (e.g., MinIO). Replace =<minio-address>:9000= with the actual address and port of your MinIO instance.
- =spark.hadoop.fs.s3a.access.key=: Your access key for the S3-compatible storage.
- =spark.hadoop.fs.s3a.secret.key=: Your secret key for the S3-compatible storage.
- =spark.hadoop.fs.s3a.path.style.access=:  This is often *required* when using MinIO or other S3-compatible storage.  It forces Spark to use path-style addressing for S3 requests, which is necessary for many non-AWS S3 implementations.

These options have become essential for me when working with Spark and S3-compatible storage. While the intricacies of S3 and Spark configurations can be confusing, explicitly setting these parameters has proven to be a reliable solution.

This is still a work in progress, but hopefully, documenting these findings will help others avoid the same pitfalls I encountered. 
